{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e6e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2165b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import rtdl\n",
    "sys.modules['rtdl.rtdl'] = rtdl\n",
    "\n",
    "from explain_icu import TheWrapper\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_explanations(folder_list, ftype=None, subset=None):\n",
    "    name_map = {\n",
    "        'wshap': 'WindowSHAP',\n",
    "        'anch': 'Anchors',\n",
    "        'comte': 'CoMTE',\n",
    "        'featpert': 'FeaturePerturbation',\n",
    "        'manualperm': 'ManualPermutation',\n",
    "        'tsr': 'TSR',\n",
    "    }\n",
    "    mapped_name = name_map[ftype]\n",
    "    \n",
    "    trial_run_exps = {}\n",
    "    for folder_name in folder_list:\n",
    "        collected_exps = []\n",
    "        for exp_num, exp_i in enumerate(subset):\n",
    "            \n",
    "            exp_package = pickle.load(open(f\"{folder_name}/{mapped_name}_encounter_1000000{exp_i}.0explanation.pkl\", \"rb\"))\n",
    "            \n",
    "            if ftype == 'wshap':\n",
    "                raw_exp = exp_package['wshap']\n",
    "            elif ftype == 'anch':\n",
    "                raw_exp = exp_package[0]\n",
    "            elif ftype == 'comte':\n",
    "                raw_exp = exp_package[0]\n",
    "                print(f\"comte 01: {exp_package[1]}\")\n",
    "            elif ftype == 'featpert':\n",
    "                raw_exp = exp_package  \n",
    "            elif ftype == 'manualperm':\n",
    "                raw_exp = exp_package \n",
    "            elif ftype == 'tsr':\n",
    "                raw_exp = exp_package\n",
    "                \n",
    "            if ftype in ['anch']:\n",
    "                print(f\"anch exp: {raw_exp}\")\n",
    "            else:\n",
    "                raw_exp = raw_exp[:,:,USED_FEATURE_LOCS]\n",
    "                raw_exp = raw_exp.squeeze()\n",
    "                print(f\"{ftype} exp shape: {raw_exp.shape}\")\n",
    "            \n",
    "            collected_exps.append(raw_exp)\n",
    "        trial_run_exps[folder_name] = collected_exps\n",
    "    return trial_run_exps\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cases_into_pdframes(EXP_SETS):\n",
    "    all_dfs = []\n",
    "    for exp_set in EXP_SETS:\n",
    "        for case_id, exp_idx in enumerate(range(len(exp_set))):\n",
    "            correct_size_data = exp_set[exp_idx].squeeze()\n",
    "            if correct_size_data.shape[-1] == 57:\n",
    "                correct_size_data = np.column_stack(([case_id for i in range(correct_size_data.shape[0])], correct_size_data))\n",
    "                correct_size_data = np.column_stack((correct_size_data, [0 for i in range(correct_size_data.shape[0])]))\n",
    "            elif correct_size_data.shape[-1] == 59:\n",
    "                pass\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Size {correct_size_data.shape[-1]} not supported\")\n",
    "\n",
    "            new_df = pd.DataFrame(correct_size_data, columns=col_names)\n",
    "            all_dfs.append(new_df)\n",
    "    return all_dfs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02649aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_different_feats(list_of_exps, list_of_origs, tolerance=0.0000001, subset=None):\n",
    "    if list_of_exps[0].shape[-1] == 57 and list_of_origs[0].shape[-1] == 59:\n",
    "        list_of_origs = [x.iloc[:, 1:-1] for x in list_of_origs]\n",
    "        list_of_exps = [x.squeeze() for x in list_of_exps]\n",
    "    elif list_of_exps[0].shape[-1] == 57 and list_of_origs[0].shape[-1] == 57:\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError(\"fail 01\")\n",
    "       \n",
    "    \n",
    "    all_changed_locs = []\n",
    "    for i, exp_data in enumerate(list_of_exps):\n",
    "        matching_orig = list_of_origs[subset[i]]\n",
    "        print(f\"exp_data: {exp_data.shape} and matching_orig: {matching_orig.shape}\")\n",
    "        by_feat_diffs = np.abs(exp_data - matching_orig).sum(axis=0)\n",
    "        changed_locs_per_exp = []\n",
    "        for f_idx, d_val in enumerate(by_feat_diffs):\n",
    "            if d_val > tolerance:\n",
    "                changed_locs_per_exp.append(f_idx)\n",
    "        all_changed_locs.append(changed_locs_per_exp)\n",
    "    return all_changed_locs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94cf745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_minmax_dict(exps_for_inst, orig_data, all_feature_names, vertical_space=0.1):\n",
    "    feature_mins = np.expand_dims(orig_data.min(axis=0), axis=1)\n",
    "    feature_maxs = np.expand_dims(orig_data.max(axis=0), axis=1)\n",
    "    \n",
    "    for explanation_of_orig in exps_for_inst:\n",
    "        exp_mins = np.expand_dims(explanation_of_orig.min(axis=0), axis=1)\n",
    "        exp_maxs = np.expand_dims(explanation_of_orig.max(axis=0), axis=1)\n",
    "        \n",
    "        combo_maxes = np.concatenate([feature_maxs, exp_maxs], axis=1)\n",
    "        feature_maxs = combo_maxes.max(axis=1)\n",
    "        \n",
    "        combo_mins = np.concatenate([feature_mins, exp_mins], axis=1)\n",
    "        feature_mins = combo_mins.min(axis=1)\n",
    "        \n",
    "    #add space\n",
    "    changes = feature_maxs - feature_mins\n",
    "    changes_scaled = changes*vertical_space\n",
    "    feature_mins = feature_mins - changes_scaled\n",
    "    feature_maxs = feature_maxs + changes_scaled\n",
    "        \n",
    "    min_max_dict = {}\n",
    "    for i in range(len(feature_mins)):\n",
    "        f_min = feature_mins[i]\n",
    "        f_max = feature_maxs[i]\n",
    "        f_name = all_feature_names[i]\n",
    "        \n",
    "        min_max_dict[f_name] = (f_min, f_max)\n",
    "    return min_max_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d656508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_counterfactual(cf_data, orig_data, changed_idxs, all_feature_names, \n",
    "                             explanation_output_folder, image_name_prefix, minmax_dict, \n",
    "                             n_img_horizontal=1, timepoint_tolerance=0.06):\n",
    "    assert len(all_feature_names) == cf_data.shape[-1] == orig_data.shape[-1], \\\n",
    "    f\"Shapes dont align; \\n\"\n",
    "    f\"len(all_feature_names)={len(all_feature_names)} \\n\"\n",
    "    f\"cf_data.shape[-1]={cf_data.shape[-1]} \\n\"\n",
    "    f\"orig_data.shape[-1]={orig_data.shape[-1]} \\n\"\n",
    "    \n",
    "    ##########\n",
    "    n_img_vertical = len(changed_idxs) // n_img_horizontal\n",
    "    remainder = len(changed_idxs) % n_img_horizontal\n",
    "    if remainder != 0:\n",
    "        n_img_vertical + 1\n",
    "        \n",
    "    fig_height = n_img_vertical * 2\n",
    "    fig_width = n_img_horizontal * 8\n",
    "    print(f\"fig_height: {fig_height} fig_width: {fig_width}\")\n",
    "    figure, ax = plt.subplots(n_img_vertical, n_img_horizontal, layout='constrained', \n",
    "                              figsize=(fig_width, fig_height))\n",
    "    if remainder > 0:\n",
    "        extra_cols_n = n_img_horizontal - remainder\n",
    "        for extra_col in range(remainder, remainder+extra_cols_n):\n",
    "            figure.delaxes(ax[n_img_vertical-1, extra_col])\n",
    "        \n",
    "    #########\n",
    "    timepoints = orig_data[:, 0]\n",
    "    max_time = timepoints.max()\n",
    "    time_diff_req = max_time * timepoint_tolerance\n",
    "    \n",
    "    time_tick_lbls = [f\"\" for x in timepoints]\n",
    "    most_recent_time = -100\n",
    "    for idx, time_val in enumerate(timepoints):\n",
    "        diff_cur_prev = time_val - most_recent_time\n",
    "        if diff_cur_prev > time_diff_req:\n",
    "            time_tick_lbls[idx] = round(time_val, 1)\n",
    "            most_recent_time = round(time_val, 1)\n",
    "    \n",
    "    #########\n",
    "    for plot_num, change_loc in enumerate(changed_idxs):\n",
    "        f_name = all_feature_names[change_loc]\n",
    "        display_name = true_display_names[f_name]\n",
    "        feat_cf_data = cf_data[:, change_loc]\n",
    "        feat_og_data = orig_data[:, change_loc]\n",
    "\n",
    "        minval = minmax_dict[f_name][0]\n",
    "        maxval = minmax_dict[f_name][1]\n",
    "        \n",
    "        if n_img_horizontal > 1:\n",
    "            main_axis = ax[plot_num // n_img_horizontal, plot_num % n_img_horizontal]\n",
    "        else:\n",
    "            main_axis = ax[plot_num // n_img_horizontal]\n",
    "        \n",
    "        if f_name == 'avpu':\n",
    "            minv, maxv = -0.5,3.5\n",
    "        if f_name == 'disoriented':\n",
    "            minv, maxv = -0.5,1.5\n",
    "        \n",
    "        main_axis.plot(timepoints, feat_og_data, color='b', label='Feature values')\n",
    "        main_axis.set_title(f\"{display_name}\")\n",
    "        main_axis.set_xticks(timepoints, labels=time_tick_lbls)\n",
    "        \n",
    "        main_axis.set_ylabel(FEAT_AXIS_DSIPLAY_NAME[f_name], color='b', loc='center')\n",
    "        main_axis.set_ylim([minval, maxval])\n",
    "        main_axis.tick_params(axis='y', colors='b')\n",
    "        \n",
    "    image_location = f\"{explanation_output_folder}/{image_name_prefix}_explanation.png\" \n",
    "    print(f\"Saving figure to {image_location}\")\n",
    "    figure.savefig(image_location)\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d36ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_pickle(\"new_input2.pkl\")\n",
    "data_to_save = pickle.load(open(\"new_background.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce87141",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_x = data_to_save['background_x']\n",
    "background_y_lbl = data_to_save['background_y_lbl']\n",
    "pad_in = data_to_save['pad_in']\n",
    "wrapper_df = data_to_save['wrapper_df']\n",
    "background_sizes = data_to_save['background_sizes']\n",
    "col_names = data_to_save['col_names']\n",
    "to_explain_shape = data_to_save['to_explain_shape']\n",
    "test_comb = data_to_save['test_comb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cefd475",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_display_names = {\n",
    " 'hours_since_admit': 'Hours Since Admit',\n",
    " 'hr': 'Heart rate',\n",
    " 'rr': 'Respiratory rate',\n",
    " 'sbp': 'SBP',\n",
    " 'dbp': 'DBP',\n",
    " 'o2sat': 'Oxygen saturation',\n",
    " 'temp_c': 'Temperature (Celsius)',\n",
    " 'avpu': 'AVPU mental status',\n",
    " 'disoriented': 'Disoriented',\n",
    " 'bmi': 'BMI',\n",
    " 'fio2_final': 'Delivered FiO2',\n",
    " 'braden_activity': 'Braden Scale - Activity',\n",
    " 'braden_friction': 'Braden Scale - Friction',\n",
    " 'braden_mobility': 'Braden Scale - Mobility',\n",
    " 'braden_moisture': 'Braden Scale - Moisture',\n",
    " 'braden_nutrition': 'Braden Scale - Nutrition',\n",
    " 'braden_sensory': 'Braden Scale - Sensory',\n",
    " 'braden_scale': 'Sum Total of Braden Scale',\n",
    " 'urine_output_sum': 'Urine output sum over 24 hours',\n",
    " 'albumin': 'Albumin',\n",
    " 'alk_phos': 'Alkaline phosphatase',\n",
    " 'anion_gap': 'Anion Gap',\n",
    " 'bands_pct': 'Bands',\n",
    " 'bili_total': 'Total bilirubin',\n",
    " 'bun': 'BUN',\n",
    " 'calcium': 'Calcium',\n",
    " 'chloride': 'Chloride',\n",
    " 'co2': 'CO2',\n",
    " 'creatinine': 'Creatinine',\n",
    " 'eosinophils_pct': 'Eosinophils',\n",
    " 'gluc_ser': 'Glucose',\n",
    " 'hb': 'Hemoglobin',\n",
    " 'inr': 'INR',\n",
    " 'lactate': 'Lactate',\n",
    " 'lipase': 'Lipase',\n",
    " 'lymphocytes_pct': 'Lymphocytes',\n",
    " 'magnesium': 'Magnesium',\n",
    " 'mcv': 'Mean Corpuscular Volume',\n",
    " 'monocytes_pct': 'Monocytes',\n",
    " 'neutrophils_pct': 'Neutrophils',\n",
    " 'pco2_art': 'PCO2 (Arterial)',\n",
    " 'pco2_ven': 'PCO2 (Venous)',\n",
    " 'ph_art': 'pH (Arterial)',\n",
    " 'ph_ven': 'pH (Venous)',\n",
    " 'phosphate': 'Inorganic Phosphate',\n",
    " 'platelet_count': 'Platelets',\n",
    " 'po2_art': 'PO2 (Arterial)',\n",
    " 'potassium': 'Potassium',\n",
    " 'ptt': 'PTT',\n",
    " 'rdw': 'RBC Distribution Width',\n",
    " 'sgot': 'SGOT',\n",
    " 'sodium': 'Sodium',\n",
    " 'total_protein': 'Total Protein',\n",
    " 'wbc': 'WBC count',\n",
    " 'age': 'Age'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae049f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TheWrapper(wrapper_df, background_sizes, col_names, to_explain_shape, n_workers=1, multiproc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84e7a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "USED_FEATURE_LOCS = list(range(1,58)) #Original is 0 to 59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_REPEATED_TRIALS = 1\n",
    "\n",
    "OUTS_RANGE_COMTE = range(0,N_REPEATED_TRIALS)\n",
    "TRIAL_FOLDER = 'final_comte/'\n",
    "COMTE_SUBSET = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "folders_COMTE = [TRIAL_FOLDER + x for x in [f'outs{y}' for y in OUTS_RANGE_COMTE]]\n",
    "\n",
    "comte_exps = load_explanations(folders_COMTE, ftype='comte', subset=COMTE_SUBSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANGE_59_TO_57 = True\n",
    "cols_names_57 = np.array(col_names[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = []\n",
    "for encid in input_data['wrapper_df']['combined_dat']['encounter_id'].unique():\n",
    "    matchdata = input_data['wrapper_df']['combined_dat'][input_data['wrapper_df']['combined_dat']['encounter_id'] == encid]\n",
    "    if CHANGE_59_TO_57:\n",
    "        matchdata = matchdata.iloc[:, 1:-1]\n",
    "    original_data.append(matchdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUSION_FIELDS = ['hours_since_admit', 'outcome_ward24hr', 'timeofday']\n",
    "EXCLUSION_LOCS = [list(cols_names_57).index(x) for x in EXCLUSION_FIELDS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc0790",
   "metadata": {},
   "source": [
    "# Generate counterfactual images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ee0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_counterfactual_images(explanation_dict, original_data, cols_names_used, subset):\n",
    "    minmax_by_trial = {}\n",
    "    comte_locs = {}\n",
    "    for trial_folder_path, list_of_exps_for_trial in explanation_dict.items():\n",
    "        per_i_diff_locs = get_different_feats(list_of_exps_for_trial, original_data, subset=subset)\n",
    "        print(f\"per_i_diff_locs: {per_i_diff_locs}\")\n",
    "        outs_ids = trial_folder_path.split('/')[-1]\n",
    "        \n",
    "        for case_id, xset in enumerate(per_i_diff_locs):\n",
    "            for x in xset:\n",
    "                if x in EXCLUSION_LOCS:\n",
    "                    print(f\"Dropping {cols_names_used[x]} at loc {x} from Case: {case_id} Folder: {outs_ids}\")\n",
    "                    xset.remove(x)\n",
    "        \n",
    "        minmax_by_case = {}\n",
    "        for case_id, case_data in enumerate(list_of_exps_for_trial):\n",
    "            minmax_vals = make_minmax_dict([case_data], original_data[case_id], cols_names_used)\n",
    "            minmax_by_case[case_id] = minmax_vals\n",
    "            \n",
    "            image_name = f\"case{subset[case_id]}_comte_{outs_ids}\"\n",
    "            \n",
    "            visualise_counterfactual(case_data, \n",
    "                                     original_data[subset[case_id]].to_numpy(), \n",
    "                                     per_i_diff_locs[case_id], \n",
    "                                     cols_names_used, \n",
    "                                     explanation_output_folder='testing01', \n",
    "                                     image_name_prefix=image_name, \n",
    "                                     minmax_dict=minmax_vals, \n",
    "                                     n_img_horizontal=1)\n",
    "        minmax_by_trial[outs_ids] = minmax_by_case\n",
    "        comte_locs[outs_ids] = per_i_diff_locs\n",
    "    return minmax_by_trial, comte_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5170e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEAT_AXIS_DSIPLAY_NAME = {\n",
    "    'hr': 'Heart rate, bpm',\n",
    "    'rr': 'RR, bpm',\n",
    "    'sbp': 'SBP, mmHg',\n",
    "    'dbp': 'DBP, mmHg',\n",
    "    'o2sat': 'O2 saturation, %',\n",
    "    'fio2_final': 'FiO2, %',\n",
    "    'lactate': 'Lactate, mmol/L',\n",
    "    'creatinine': 'Creatinine mg/dL',\n",
    "    'bili_total': 'Bilirubin total, mg/dL',\n",
    "    'sgot': 'SGOT, U/L',\n",
    "    'temp_c': 'Temperature, \\u00B0C',\n",
    "    'avpu': 'AVPU, status',\n",
    "    'disoriented': 'Disoriented, status',\n",
    "    'bmi': 'BMI, kg/m\\u00b2',\n",
    "    'braden_moisture': 'Braden Scale - Moisture, score',\n",
    "    'braden_nutrition': 'Braden Scale - Nutrition, score',\n",
    "    'braden_activity': 'Braden Scale - Activity, score',\n",
    "    'bands_pct': 'PCT',\n",
    "    'hb': 'HB',\n",
    "    'lymphocytes_pct': 'lymphocytes_pct',\n",
    "    'mcv': 'mcv',\n",
    "    'mcv2': 'mcv2',\n",
    "    'braden_friction': 'braden_friction',\n",
    "    'gluc_ser': 'gluc_ser',\n",
    "    'inr': 'inr',\n",
    "    'sodium': 'sodium',\n",
    "    'age': 'age',\n",
    "    'bun': 'bun',\n",
    "    'braden_scale': 'Braden Scale - Total, score',\n",
    "    'platelet_count': 'platelet_count',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319dfab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_dict, comte_locs = gen_counterfactual_images(comte_exps, original_data, cols_names_57, subset=COMTE_SUBSET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a73db",
   "metadata": {},
   "source": [
    "# Running counterfactuals through model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fb62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = model(input_data['background_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb972543",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(input_data['background_x']):\n",
    "    inst_len = input_data['background_sizes'][i]\n",
    "    model.to_explain_shape = (inst_len, 59)\n",
    "    val = model(x[-inst_len:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864f796",
   "metadata": {},
   "source": [
    "# WindowSHAP - Making visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INSTANCES_EXPLAINED = 10\n",
    "N_REPEATED_TRIALS = 1\n",
    "\n",
    "OUTS_RANGE_WSHAP = range(0,1)\n",
    "OUTS_RANGE_ANCH = range(0,1)\n",
    "OUTS_RANGE_FEATPERM = range(0,N_REPEATED_TRIALS)\n",
    "OUTS_RANGE_TSR = range(0,N_REPEATED_TRIALS)\n",
    "OUTS_RANGE_MPERM = range(0,N_REPEATED_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55551436",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pickle.load(open(\"explanation_outputs/outs10/WindowSHAP_encounter_10000005.0explanation.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efe49e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_ANCHORS = ['trial11_vis_outs/' + x for x in [f'outs{y}' for y in OUTS_RANGE_ANCH]]\n",
    "\n",
    "folders_WSHAP = ['trial13_vis_outs/' + x for x in [f'outs{y}' for y in OUTS_RANGE_WSHAP]]\n",
    "\n",
    "folders_FEATPERM = ['trial9_vis_outs/' + x for x in [f'outs{y}' for y in OUTS_RANGE_FEATPERM]]\n",
    "\n",
    "folders_TSR = ['trial9_vis_outs/' + x for x in [f'outs{y}' for y in OUTS_RANGE_TSR]]\n",
    "\n",
    "folders_MPERM = ['trial9_vis_outs/' + x for x in [f'outs{y}' for y in OUTS_RANGE_MPERM]]\n",
    "\n",
    "wshap_exps = load_explanations(folders_WSHAP, ftype='wshap')\n",
    "\n",
    "manualperm_exps = load_explanations(folders_MPERM, ftype='manualperm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_featattrib(np_exp_tensor, k=10):\n",
    "    per_feat_vals = np.abs(np_exp_tensor.squeeze()).sum(axis=0)\n",
    "    top_k_locs = np.argpartition(per_feat_vals, -k)[-k:]\n",
    "    print(f\"k: {k} and then top_k_locs: {top_k_locs}\")\n",
    "    top_vals = np_exp_tensor[:,top_k_locs]\n",
    "    \n",
    "    return top_k_locs, top_vals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c57c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_featureattribution(importance_data, orig_data, top_k_locs, all_feature_names, \n",
    "                             explanation_output_folder, image_name_prefix, minmax_dict, \n",
    "                             n_img_horizontal=1, timepoint_tolerance=0.06, orig_only=False):\n",
    "    assert len(all_feature_names) == importance_data.shape[-1] == orig_data.shape[-1], \\\n",
    "    f\"Shapes dont align; len(all_feature_names)={len(all_feature_names)} importance_data.shape[-1]={importance_data.shape[-1]} orig_data.shape[-1]={orig_data.shape[-1]} \\n\"\n",
    "    \n",
    "    ##########\n",
    "    n_img_vertical = len(top_k_locs) // n_img_horizontal\n",
    "    remainder = len(top_k_locs) % n_img_horizontal\n",
    "    if remainder != 0:\n",
    "        n_img_vertical + 1\n",
    "        \n",
    "    fig_height = n_img_vertical * 2\n",
    "    fig_width = n_img_horizontal * 8\n",
    "    figure, ax = plt.subplots(n_img_vertical, n_img_horizontal, layout='constrained', \n",
    "                              figsize=(fig_width, fig_height))\n",
    "    \n",
    "    if remainder > 0:\n",
    "        extra_cols_n = n_img_horizontal - remainder\n",
    "        for extra_col in range(remainder, remainder+extra_cols_n):\n",
    "            figure.delaxes(ax[n_img_vertical-1, extra_col])\n",
    "        \n",
    "    #########\n",
    "    timepoints = orig_data[:, 0]\n",
    "    max_time = timepoints.max()\n",
    "    time_diff_req = max_time * timepoint_tolerance\n",
    "    \n",
    "    time_tick_lbls = [f\"\" for x in timepoints]\n",
    "    most_recent_time = -100\n",
    "    for idx, time_val in enumerate(timepoints):\n",
    "        diff_cur_prev = time_val - most_recent_time\n",
    "        if diff_cur_prev > time_diff_req:\n",
    "            time_tick_lbls[idx] = round(time_val, 1)\n",
    "            most_recent_time = round(time_val, 1)    \n",
    "    \n",
    "    ######### Normalize importance data\n",
    "    importance_data = (importance_data - importance_data.mean()) / importance_data.std()\n",
    "    \n",
    "    #########\n",
    "    min_importance = importance_data[:,top_k_locs].min()\n",
    "    min_importance = min_importance - (np.abs(min_importance) * 0.1)\n",
    "    max_importance = importance_data[:,top_k_locs].max()\n",
    "    max_importance = max_importance + (np.abs(max_importance) * 0.1)\n",
    "        \n",
    "    for plot_num, top_loc in enumerate(top_k_locs):\n",
    "        f_name = all_feature_names[top_loc]\n",
    "        display_name = true_display_names[f_name]\n",
    "        feat_importance_data = importance_data[:, top_loc]\n",
    "        feat_og_data = orig_data[:, top_loc]\n",
    "        \n",
    "        bar_width = 10/feat_og_data.shape[0]\n",
    "        \n",
    "        minval = minmax_dict[f_name][0]\n",
    "        maxval = minmax_dict[f_name][1]\n",
    "        print(f\"display_name: {display_name} min: {minval} max: {maxval}\")\n",
    "        \n",
    "        if n_img_horizontal > 1:\n",
    "            main_axis = ax[plot_num // n_img_horizontal, plot_num % n_img_horizontal]\n",
    "        else:\n",
    "            main_axis = ax[plot_num // n_img_horizontal]\n",
    "        \n",
    "        if not orig_only:\n",
    "            overlay_plot = main_axis.twinx()\n",
    "            overlay_plot.axhline(y=0, color='red', linestyle=\":\", alpha=0.25)\n",
    "            overlay_plot.set_zorder(1)\n",
    "            main_axis.set_zorder(2)\n",
    "            main_axis.patch.set_visible(False)\n",
    "        \n",
    "        main_axis.plot(timepoints, feat_og_data, color='b', label='Feature values')\n",
    "        main_axis.set_title(f\"{display_name}\")\n",
    "        main_axis.set_xticks(timepoints, labels=time_tick_lbls)\n",
    "        \n",
    "        if f_name == 'avpu':\n",
    "            minv, maxv = -0.5,3.5\n",
    "            main_axis.set_yticks([0,1,2,3])\n",
    "            main_axis.set_yticklabels(['Alert', 'Responds to Voice', 'Responds to Pain', 'Unresponsive'])\n",
    "        if f_name == 'disoriented':\n",
    "            minv, maxv = -0.5,1.5\n",
    "            overlay_plot.set_yticks([0,1])\n",
    "            overlay_plot.set_yticklabels(['No', 'Yes'])\n",
    "        \n",
    "        main_axis.set_ylabel(FEAT_AXIS_DSIPLAY_NAME[f_name], color='b', loc='center')\n",
    "        main_axis.set_ylim([minval, maxval])\n",
    "        main_axis.tick_params(axis='y', colors='b')\n",
    "\n",
    "        if not orig_only:\n",
    "            overlay_plot.bar(timepoints, feat_importance_data, color='r', label='Importance Values', \n",
    "                             linestyle='dashed', width=bar_width)\n",
    "            overlay_plot.set_ylabel(\"Importance Values\", color='r')\n",
    "            overlay_plot.set_ylim([min_importance, max_importance])\n",
    "            overlay_plot.tick_params(axis='y', colors='r')\n",
    "        \n",
    "    image_location = f\"{explanation_output_folder}/{image_name_prefix}_explanation.png\" \n",
    "    print(f\"Saving figure to {image_location}\")\n",
    "    figure.savefig(image_location)\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13096a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_featureattribution_images(explanation_dict, original_data, cols_names_used, minmax_dict, comte_locs, img_prefix=\"wshap\", orig_only=False):\n",
    "    for trial_folder_path, list_of_exps_for_trial in explanation_dict.items():\n",
    "        \n",
    "        outs_ids = trial_folder_path.split('/')[-1]\n",
    "        minmax_vals_for_trial = minmax_dict[outs_ids]\n",
    "        trial_comte_locs = comte_locs[outs_ids]\n",
    "        \n",
    "        for case_id, case_data in enumerate(list_of_exps_for_trial):\n",
    "            minmax_for_case = minmax_vals_for_trial[case_id]\n",
    "            \n",
    "            image_name = f\"case{case_id}_{img_prefix}_{outs_ids}\"\n",
    "            \n",
    "            case_top_k_locs, _ = get_top_k_featattrib(np_exp_tensor=case_data, k=5)\n",
    "            filtered_case_locs = []\n",
    "            for item in case_top_k_locs:\n",
    "                if item not in EXCLUSION_LOCS:\n",
    "                    filtered_case_locs.append(item)\n",
    "                    \n",
    "            case_comte_locs = trial_comte_locs[case_id]\n",
    "            case_top_k_locs = np.array(list(set(filtered_case_locs + case_comte_locs)))\n",
    "        \n",
    "            importance_vals = np.sum(np.abs(case_data), axis=0)[case_top_k_locs]\n",
    "            order_locs = np.argsort(importance_vals)\n",
    "            ordered_fs = [case_top_k_locs[x] for x in order_locs]\n",
    "            ordered_fs.reverse()\n",
    "            \n",
    "            visualise_featureattribution(case_data, \n",
    "                                     original_data[case_id].to_numpy(), \n",
    "                                     ordered_fs, \n",
    "                                     cols_names_used, \n",
    "                                     explanation_output_folder='testing01', \n",
    "                                     image_name_prefix=image_name, \n",
    "                                     minmax_dict=minmax_for_case, \n",
    "                                     n_img_horizontal=1,\n",
    "                                     orig_only=orig_only)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_featureattribution_images(wshap_exps, original_data, cols_names_57, minmax_dict, comte_locs,\n",
    "                             orig_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f0d32d",
   "metadata": {},
   "source": [
    "# Evaluating Anchor Explanations on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f68980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = real_data['combined_dat']['encounter_id'].unique()\n",
    "total_len = real_data['combined_dat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7150a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_uniques = []\n",
    "latest_id = all_ids[0]\n",
    "\n",
    "current_sect = []\n",
    "for i, row_data in real_data['combined_dat'].iterrows():\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"i: {i}\")\n",
    "    if row_data['encounter_id'] == latest_id:\n",
    "        current_sect.append(row_data)\n",
    "    else:\n",
    "        try:\n",
    "            this_id_group = pd.concat(current_sect, axis=1).transpose()\n",
    "            list_of_uniques.append(this_id_group)\n",
    "            print(f\"Added patient {latest_id} with {this_id_group.shape} rows\")\n",
    "\n",
    "            current_sect = [row_data]\n",
    "            latest_id = row_data['encounter_id']\n",
    "        except Exception as ex:\n",
    "            print(f\"Error: {ex}\")\n",
    "            current_sect = [row_data]\n",
    "            latest_id = row_data['encounter_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7ee6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_run = list(anchor_exps.keys())[0]\n",
    "instance_num = 1\n",
    "MAX_NUM = 1000\n",
    "\n",
    "anchor_rules = anchor_exps[folder_run][instance_num]['names']\n",
    "\n",
    "def eval_rules(list_of_uniques):\n",
    "    rules = []\n",
    "    for rule in anchor_rules:\n",
    "        feat_time, sign_char, val = rule.split(\" \")\n",
    "        feat_name = \"_\".join(feat_time.split(\"_\")[:-1])\n",
    "        time = int(feat_time.split(\"_\")[-1])\n",
    "        feat_loc = col_names.index(feat_name)\n",
    "\n",
    "        rules.append([feat_name, feat_loc, time, sign_char, val])\n",
    "\n",
    "    total_n_instances = 0\n",
    "    n_inst_rule_applies = 0\n",
    "    n_inst_rule_applies_pos = 0\n",
    "    n_inst_rule_applies_neg = 0\n",
    "    n_pred_pos = 0\n",
    "    n_rule_fails_but_pos = 0\n",
    "    \n",
    "\n",
    "    for i, test_instance in enumerate(list_of_uniques):\n",
    "        rule_applies = True\n",
    "        \n",
    "        if i >= MAX_NUM:\n",
    "            break\n",
    "        \n",
    "        np_inst = test_instance.to_numpy()\n",
    "        model.to_explain_shape = np_inst.shape\n",
    "        prediction = model(np_inst)\n",
    "        pred_lbl = np.argmax(prediction)\n",
    "        if pred_lbl == 1:\n",
    "            n_pred_pos += 1\n",
    "\n",
    "        total_n_instances += 1\n",
    "\n",
    "        for r_data in rules:\n",
    "            r_time = r_data[2]\n",
    "            r_f_name = r_data[0]\n",
    "            r_sign_char = r_data[3]\n",
    "            r_val = r_data[4]\n",
    "\n",
    "            if test_instance.shape[0] > r_time:\n",
    "                if r_sign_char == '>':\n",
    "                    if not test_instance.iloc[r_time][r_f_name] > float(r_val):\n",
    "                        rule_applies = False\n",
    "                elif r_sign_char == '<':\n",
    "                    if not test_instance.iloc[r_time][r_f_name] < float(r_val):\n",
    "                        rule_applies = False\n",
    "            else:\n",
    "                rule_applies = False\n",
    "\n",
    "        if rule_applies:\n",
    "            n_inst_rule_applies += 1\n",
    "\n",
    "            if pred_lbl == 1:\n",
    "                n_inst_rule_applies_pos += 1\n",
    "            else:\n",
    "                n_inst_rule_applies_neg += 1\n",
    "        else:\n",
    "            n_rule_fails_but_pos += 1\n",
    "                \n",
    "    return total_n_instances, \\\n",
    "            n_inst_rule_applies, \\\n",
    "            n_inst_rule_applies_pos, \\\n",
    "            n_inst_rule_applies_neg, \\\n",
    "            n_pred_pos, \\\n",
    "            n_rule_fails_but_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea0a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_n_instances, n_inst_rule_applies, n_inst_rule_applies_pos, n_inst_rule_applies_neg, n_pred_pos, n_rule_fails_but_pos = eval_rules(list_of_uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = n_inst_rule_applies_pos / (n_inst_rule_applies_pos + n_inst_rule_applies_neg)\n",
    "coverage = n_inst_rule_applies / total_n_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba7460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#roughly 5% of instances should have positive label\n",
    "print(f\"applied_pos to applied ratio/PRECISION: {precision}\")\n",
    "print(f\"COVERAGE: {coverage}\")\n",
    "print(f\"n_inst_rule_applies: {n_inst_rule_applies}, n_inst_rule_applies_pos:{n_inst_rule_applies_pos}, n_inst_rule_applies_neg:{n_inst_rule_applies_neg}\")\n",
    "print(f\"total_n_instances: {total_n_instances}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc85e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_exs = pickle.load(open(\"new_background.pkl\", \"rb\"))\n",
    "b_matches = [pd.DataFrame(x, columns=col_names) for x in back_exs['background_x']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed6df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot1, app1, apppos1, appneg1 = eval_rules(b_matches)\n",
    "cov = app1/tot1\n",
    "prc = apppos1/app1\n",
    "\n",
    "print(f\"Precision: {prc} \\t\\t Coverage: {cov}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37546131",
   "metadata": {},
   "source": [
    "# Explain Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IG_importances = pickle.load(open('trial22_vis_outs/IntegratedGrad', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_exps = {'trial22_vis_outs/outs0': [v['importance'].to_numpy()[:, 1:-1] for k,v in enumerate(IG_importances)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c756ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "comte_plus_anchors = copy.deepcopy(comte_locs)\n",
    "#defined via manual examination of explanations\n",
    "anchor_feat_locs = [\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    ['lactate', 'rr', 'hr', 'braden_scale', 'fio2_final'],\n",
    "    ['rr', 'fio2_final'],\n",
    "    ['hr', 'braden_nutrition', 'fio2_final'],\n",
    "    ['rr', 'fio2_final'],\n",
    "    ['rr', 'fio2_final'],\n",
    "    [],\n",
    "    [],\n",
    "]\n",
    "\n",
    "for il, caselist in enumerate(comte_plus_anchors['outs0']):\n",
    "    for af in anchor_feat_locs[il]:\n",
    "        if list(cols_names_57).index(af) not in caselist:\n",
    "            caselist.append(list(cols_names_57).index(af))\n",
    "comte_plus_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_featureattribution_images(ig_exps, original_data, cols_names_57, minmax_dict, comte_locs, img_prefix=\"IG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90487a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_background = pickle.load(open('real_background.pkl', 'rb'))\n",
    "real_input2 = pickle.load(open('new_input2.pkl', 'rb'))\n",
    "\n",
    "real_input2['orig_dat_noise'][['encounter_id', 'hours_since_admit', 'creatinine']][real_input2['orig_dat_noise']['creatinine'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7073234",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_input2['orig_dat_noise'][['encounter_id', 'hours_since_admit', 'creatinine']][real_input2['orig_dat_noise']['creatinine'].notnull()]\n",
    "                                                                                                                 \n",
    "testinginput3 = pickle.load(open('2000_01_2_new_pred_dict_noise_20241031.pickle', 'rb'))\n",
    "check3 = testinginput3['orig_dat_noise'][['encounter_id', 'hours_since_admit', 'creatinine']][testinginput3['orig_dat_noise']['creatinine'].notnull()]\n",
    "print(f\"without any: {len(check3['encounter_id'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a6057",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = pd.read_pickle('2000_high_new_pred_dict_noise_20241031.pickle')\n",
    "tempid = 'EAAAL91N'\n",
    "new_input['orig_dat_sample'][['encounter_id', 'hours_since_admit', 'creatinine']][new_input['orig_dat_sample']['encounter_id'] == tempid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a461841",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_set = pd.read_pickle('copy_deterioration_data_20240417-1657_combined_input_dict.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c239ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rec_length = 28\n",
    "complied_records = []\n",
    "\n",
    "working_id = all_data_set['combined_dat'].iloc[0]['encounter_id']\n",
    "working_data = []\n",
    "\n",
    "for row_i in range(len(all_data_set['combined_dat'])):\n",
    "    if row_i % 1000 == 0:\n",
    "        print(f\"On {row_i}/{len(all_data_set['combined_dat'])} count: {len(complied_records)}\")\n",
    "    \n",
    "    row = all_data_set['combined_dat'].iloc[row_i]\n",
    "    row_id = row['encounter_id']\n",
    "    \n",
    "    if row_id == working_id:\n",
    "        working_data.append(row)\n",
    "    else:\n",
    "        pd_record = pd.concat(working_data, axis=1).transpose()\n",
    "        \n",
    "        if len(pd_record) >= min_rec_length:\n",
    "            complied_records.append(pd_record.iloc[-28:, :])\n",
    "            \n",
    "        working_id = row_id\n",
    "        working_data = [row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38224b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_recs = np.stack(complied_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a20eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\"braden_nutrition_14 = 2\"]\n",
    "which_case = 5\n",
    "\n",
    "eid = real_input2['orig_dat_noise']['encounter_id'].unique()[which_case]\n",
    "\n",
    "match = real_input2['orig_dat_noise'][real_input2['orig_dat_noise']['encounter_id'] == eid]\n",
    "local_background = stacked_recs[:, -match.shape[0]:, :]\n",
    "start_size = local_background.shape[0]\n",
    "\n",
    "for c in conditions:\n",
    "    f_t, symbol, val = c.split(\" \")\n",
    "    feat, time = f_t.rsplit(\"_\", 1)\n",
    "    col_loc = col_names.index(feat)\n",
    "\n",
    "    if symbol == \">\":\n",
    "        applied_conds = local_background[:, int(time), col_loc] > float(val)\n",
    "    elif symbol == \"<=\":\n",
    "        applied_conds = local_background[:, int(time), col_loc] <= float(val)\n",
    "    elif symbol == \"=\":\n",
    "        applied_conds = local_background[:, int(time), col_loc] = float(val)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Err\")\n",
    "\n",
    "    true_idxs = np.where(applied_conds)\n",
    "    local_background = local_background[true_idxs]\n",
    "    print(f\"Applying condition {feat} at time {time} {symbol} {val}. New size: {local_background.shape}\")\n",
    "    \n",
    "model.to_explain_shape = match.shape\n",
    "if local_background.shape[0] > 1000:\n",
    "    res_holder = []\n",
    "    for chunk in np.array_split(local_background, (local_background.shape[0]//1000)+1):\n",
    "        tempr = model(chunk)\n",
    "        res_holder.append(tempr)\n",
    "    res = np.concatenate(res_holder, axis=0)\n",
    "else:\n",
    "    res = model(local_background)\n",
    "res = res[:,1] > res[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3d92f",
   "metadata": {},
   "source": [
    "### Checking Anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1806e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pickle.load(open('old_outputs/trial20_vis_outs/outs0/Anchors_encounter_10000005.0explanation.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f2013",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempins = pickle.load(open('new_input2.pkl', 'rb'))\n",
    "tempins.keys()\n",
    "tempins['orig_dat_noise']['encounter_id'].unique()\n",
    "\n",
    "ids = ['EAAAMYJB', 'EAAAIG9E', 'EAAAJ8PR', 'EAAAM4E5', 'EAAANTEY',\n",
    "       'EAAAKMHX', 'EAAAL0KH', 'EAAAL91N', 'EAAANMQ0', 'EAAALRCT']\n",
    "\n",
    "case_id = 5\n",
    "holder = tempins['orig_dat_noise'][tempins['orig_dat_noise']['encounter_id'] == ids[case_id]]\n",
    "for x in list(range(holder.shape[0])):\n",
    "    t = holder['hours_since_admit'].iloc[x]\n",
    "    print(f\"Mapping written time {x} to real time {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempins = pickle.load(open('new_input2.pkl', 'rb'))\n",
    "unique_ids = tempins['orig_dat_noise']['encounter_id'].unique()\n",
    "\n",
    "care_about_cols_labs = ['hours_since_admit', 'hr', 'rr', 'sbp', 'dbp', 'o2sat', 'temp_c', 'avpu', 'disoriented',\n",
    "       'fio2_final', 'braden_nutrition', 'braden_scale', 'lactate', 'creatinine']\n",
    "\n",
    "tempins['orig_dat_noise']['hours_since_admit'] = tempins['orig_dat_noise']['hours_since_admit'].round(1)\n",
    "\n",
    "tempins['orig_dat_noise']['avpu'].loc[tempins['orig_dat_noise']['avpu'] == 0] = 'Alert'\n",
    "tempins['orig_dat_noise']['avpu'].loc[tempins['orig_dat_noise']['avpu'] == 1] = 'Responds to Voice'\n",
    "tempins['orig_dat_noise']['avpu'].loc[tempins['orig_dat_noise']['avpu'] == 2] = 'Responds to Pain'\n",
    "tempins['orig_dat_noise']['avpu'].loc[tempins['orig_dat_noise']['avpu'] == 3] = 'Unresponsive'\n",
    "\n",
    "tempins['orig_dat_noise']['disoriented'].loc[tempins['orig_dat_noise']['disoriented'] == 0] = 'No'\n",
    "tempins['orig_dat_noise']['disoriented'].loc[tempins['orig_dat_noise']['disoriented'] == 1] = 'Yes'\n",
    "\n",
    "for inum, eid in enumerate(unique_ids):\n",
    "    match_recs = tempins['orig_dat_noise'][tempins['orig_dat_noise']['encounter_id'] == eid]\n",
    "    desired_data1 = match_recs[care_about_cols_labs].rename(columns=true_display_names).transpose()\n",
    "    \n",
    "    #drop empty cols\n",
    "    to_drop = []\n",
    "    for colname in desired_data1.columns:\n",
    "        if pd.isnull(desired_data1[colname][1:]).values.all():\n",
    "            to_drop.append(colname)\n",
    "    print(f\"to_drop: {to_drop}\")\n",
    "    desired_data1 = desired_data1.drop(to_drop, axis=1)\n",
    "    \n",
    "    #merge cols in same hour if they have no conflicts\n",
    "    to_drop2 = []\n",
    "    for cloc in range(len(desired_data1.columns)-1):\n",
    "        thiscol = desired_data1.iloc[:, cloc]\n",
    "        nextcol = desired_data1.iloc[:, cloc+1]\n",
    "        if thiscol.iloc[0] - nextcol.iloc[0] < 1:\n",
    "            thiscol_nan = pd.isnull(thiscol)\n",
    "            nextcol_nan = pd.isnull(nextcol)\n",
    "            col_holder = pd.Series(np.full(14, np.nan))\n",
    "            if (thiscol_nan.astype(int) + nextcol_nan.astype(int) < 2).all():\n",
    "                for ix, pair in enumerate(zip(thiscol, nextcol)):\n",
    "                    val1, val2 = pair[0], pair[1]\n",
    "                    try:\n",
    "                        col_holder.iloc[ix] = val1 if not np.isnan(val1) else val2\n",
    "                    except TypeError as ex:\n",
    "                        col_holder.iloc[ix] = val1 if not val1 == 'nan' else val2\n",
    "                to_drop2.append(cloc+1)\n",
    "                desired_data1.iloc[:, cloc] = col_holder\n",
    "    \n",
    "    desired_data1 = desired_data1.fillna(value=\"-\")\n",
    "    desired_data1.to_csv(f'testing01/case{inum}_labs_subsetvars1.csv', header=False)\n",
    "    \n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf7c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
