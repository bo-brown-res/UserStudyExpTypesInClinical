{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe3ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas\n",
    "import numpy\n",
    "import itertools\n",
    "\n",
    "from explain_visuals import plot_original_overlap_counterfactual\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from explain_visuals import plot_original_overlap_counterfactual,plot_original_line_with_vals, plot_original_line_with_dots\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc10a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = pandas.read_pickle(\"new_input.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = datafile['wrapper_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be92c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = list(temp['combined_dat'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae969c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_names = {\n",
    " 'hours_since_admit': 'Hours Since Admit',\n",
    " 'hr': 'Heart rate',\n",
    " 'rr': 'Respiratory rate',\n",
    " 'sbp': 'SBP',\n",
    " 'dbp': 'DBP',\n",
    " 'o2sat': 'Oxygen saturation',\n",
    " 'temp_c': 'Temperature (Celsius)',\n",
    " 'avpu': 'AVPU mental status',\n",
    " 'disoriented': 'Disoriented',\n",
    " 'bmi': 'BMI',\n",
    " 'fio2_final': 'Delivered FiO2',\n",
    " 'braden_activity': 'Braden Scale - Activity',\n",
    " 'braden_friction': 'Braden Scale - Friction',\n",
    " 'braden_mobility': 'Braden Scale - Mobility',\n",
    " 'braden_moisture': 'Braden Scale - Moisture',\n",
    " 'braden_nutrition': 'Braden Scale - Nutrition',\n",
    " 'braden_sensory': 'Braden Scale - Sensory',\n",
    " 'braden_scale': 'Sum Total of Braden Scale',\n",
    " 'urine_output_sum': 'Urine output sum over 24 hours',\n",
    " 'albumin': 'Albumin',\n",
    " 'alk_phos': 'Alkaline phosphatase',\n",
    " 'anion_gap': 'Anion Gap',\n",
    " 'bands_pct': 'Bands',\n",
    " 'bili_total': 'Total bilirubin',\n",
    " 'bun': 'BUN',\n",
    " 'calcium': 'Calcium',\n",
    " 'chloride': 'Chloride',\n",
    " 'co2': 'CO2',\n",
    " 'creatinine': 'Creatinine',\n",
    " 'eosinophils_pct': 'Eosinophils',\n",
    " 'gluc_ser': 'Glucose',\n",
    " 'hb': 'Hemoglobin',\n",
    " 'inr': 'INR',\n",
    " 'lactate': 'Lactate',\n",
    " 'lipase': 'Lipase',\n",
    " 'lymphocytes_pct': 'Lymphocytes',\n",
    " 'magnesium': 'Magnesium',\n",
    " 'mcv': 'Mean Corpuscular Volume',\n",
    " 'monocytes_pct': 'Monocytes',\n",
    " 'neutrophils_pct': 'Neutrophils',\n",
    " 'pco2_art': 'PCO2 (Arterial)',\n",
    " 'pco2_ven': 'PCO2 (Venous)',\n",
    " 'ph_art': 'pH (Arterial)',\n",
    " 'ph_ven': 'pH (Venous)',\n",
    " 'phosphate': 'Inorganic Phosphate',\n",
    " 'platelet_count': 'Platelets',\n",
    " 'po2_art': 'PO2 (Arterial)',\n",
    " 'potassium': 'Potassium',\n",
    " 'ptt': 'PTT',\n",
    " 'rdw': 'RBC Distribution Width',\n",
    " 'sgot': 'SGOT',\n",
    " 'sodium': 'Sodium',\n",
    " 'total_protein': 'Total Protein',\n",
    " 'wbc': 'WBC count',\n",
    " 'age': 'Age'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0995d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_int_map = {k: int(k+10000000) for k in temp['combined_dat']['encounter_id'].unique()}\n",
    "\n",
    "def find_records(all_data, list_encids):\n",
    "    return all_data[all_data['encounter_id'].isin(list_encids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d791e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_comte_folders = ['trial9_vis_outs/' + x for x in ['outs0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ca9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_anch_folders = ['trial8_vis_outs/' + x for x in ['outs0', 'outs1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dcaab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_wshap_folders = ['trial9_vis_outs/' + x for x in ['outs0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24bc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_WSHAP_DICT = False\n",
    "ACTUAL_EXP_NAME = 'FeaturePerturbation' #WindowSHAP #ManualPermutation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca7bac",
   "metadata": {},
   "source": [
    "# ACCROSS ALL COUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8097b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_MAX_INT = 3\n",
    "WINDOW_SHAP_N_TOP = 5\n",
    "N_CASES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the explanations\n",
    "across_all_counts = {'anchors':{}, 'comte':{}, 'windowshap':{}}\n",
    "for foldernum, loc_folder in enumerate(list_of_comte_folders):\n",
    "    for i in range(0, FOLDER_MAX_INT):\n",
    "        \n",
    "        ################################### Anchors ###################################\n",
    "        anch_folder = list_of_anch_folders[foldernum]\n",
    "        tf = open(f\"{anch_folder}/Anchors_encounter_1000000{i}.0explanation.pkl\", \"rb\")\n",
    "        anchor = pickle.load(tf)[0]\n",
    "\n",
    "        for acn in anchor['names']:\n",
    "            f_name = acn.split(\" \")[0]\n",
    "            if f_name in across_all_counts['anchors'].keys():\n",
    "                across_all_counts['anchors'][f_name] += 1\n",
    "            else:\n",
    "                across_all_counts['anchors'][f_name] = 1\n",
    "\n",
    "        ################################### COMTE ###################################\n",
    "        curid = i\n",
    "        matching_records = find_records(temp['combined_dat'], [curid])\n",
    "        matching_records['encounter_id'] = i\n",
    "        orig = matching_records.to_numpy()\n",
    "\n",
    "        tf = open(f\"{loc_folder}/CoMTE_encounter_1000000{i}.0explanation.pkl\", \"rb\")\n",
    "        comte = pickle.load(tf)[0].squeeze()\n",
    "        comte = numpy.concatenate([orig[:,:1], comte, orig[:,-1:]], axis=1)\n",
    "\n",
    "        diffs = numpy.abs(numpy.sum(orig - comte, axis=0))\n",
    "        for fid, f in enumerate(diffs):\n",
    "            if f > 0.0001:\n",
    "                if col_names[fid] in across_all_counts['comte'].keys():\n",
    "                    across_all_counts['comte'][col_names[fid]] += 1\n",
    "                else:\n",
    "                    across_all_counts['comte'][col_names[fid]] = 1\n",
    "\n",
    "\n",
    "        ################################### WINDOWSHAP ###################################\n",
    "        wshap_folder = list_of_wshap_folders[foldernum]\n",
    "        tf = open(f\"{wshap_folder}/{ACTUAL_EXP_NAME}_encounter_1000000{i}.0explanation.pkl\", \"rb\")\n",
    "        if IS_WSHAP_DICT:\n",
    "            wshap = pickle.load(tf)['wshap'][0]\n",
    "        else:\n",
    "            wshap = pickle.load(tf)[0]\n",
    "            wshap = wshap - wshap.mean(axis=0)\n",
    "        wshap = numpy.abs(wshap)\n",
    "        tot_wshap = wshap.mean(axis=0)\n",
    "\n",
    "        top10 = numpy.argpartition(tot_wshap, -WINDOW_SHAP_N_TOP)[-WINDOW_SHAP_N_TOP:]\n",
    "        for top_id in top10:\n",
    "            if col_names[top_id] in across_all_counts['windowshap'].keys():\n",
    "                    across_all_counts['windowshap'][col_names[top_id]] += 1\n",
    "            else:\n",
    "                across_all_counts['windowshap'][col_names[top_id]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e7443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting explanation objects\n",
    "wsc = across_all_counts['windowshap']\n",
    "ws_counts = {k:v for k,v in sorted(wsc.items(), key=lambda pair: pair[1])}\n",
    "\n",
    "cc = across_all_counts['comte']\n",
    "c_counts = {k:v for k,v in sorted(cc.items(), key=lambda pair: pair[1])}\n",
    "\n",
    "new_anchors = {}\n",
    "for lbl,v in across_all_counts['anchors'].items():\n",
    "    terms = \"_\".join(lbl.split(\"_\")[:-1])\n",
    "    if terms in new_anchors.keys():\n",
    "        new_anchors[terms] += v\n",
    "    else:\n",
    "        new_anchors[terms] = v\n",
    "a_count = {k:v for k,v in sorted(new_anchors.items(), key=lambda pair: pair[1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3624a78",
   "metadata": {},
   "source": [
    "# ACROSS CASE COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b31950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading explanations for cases\n",
    "across_case_counts = {f'case_{j}':{'anchors':{}, 'comte':{}, 'windowshap':{}} for j in range(0,N_CASES)}\n",
    "for foldernum, loc_folder in enumerate(list_of_comte_folders):\n",
    "    for i in range(0, N_CASES):\n",
    "        ################################### Anchors ###################################\n",
    "        anch_folder = list_of_anch_folders[foldernum]\n",
    "        tf = open(f\"{anch_folder}/Anchors_encounter_1000000{i}.0explanation.pkl\", \"rb\")\n",
    "        anchor = pickle.load(tf)[0]\n",
    "\n",
    "        for acn in anchor['names']:\n",
    "            f_name = acn.split(\" \")[0]\n",
    "            f_name = \"_\".join(f_name.split(\"_\")[:-1])\n",
    "            if f_name not in ['encounter_id', 'cuis']:\n",
    "                if f_name in across_case_counts[f'case_{i}']['anchors'].keys():\n",
    "                    across_case_counts[f'case_{i}']['anchors'][f_name] += 1\n",
    "                else:\n",
    "                    across_case_counts[f'case_{i}']['anchors'][f_name] = 1\n",
    "\n",
    "        ################################### COMTE ###################################\n",
    "        curid = i\n",
    "        matching_records = find_records(temp['combined_dat'], [curid])\n",
    "        matching_records['encounter_id'] = i\n",
    "        orig = matching_records.to_numpy()\n",
    "\n",
    "        tf = open(f\"{loc_folder}/CoMTE_encounter_1000000{i}.0explanation.pkl\", \"rb\")\n",
    "        comte = pickle.load(tf)[0].squeeze()\n",
    "        comte = numpy.concatenate([orig[:,:1], comte, orig[:,-1:]], axis=1)\n",
    "\n",
    "        diffs = numpy.abs(numpy.sum(orig - comte, axis=0))\n",
    "        for fid, f in enumerate(diffs):\n",
    "            if f > 0.0001:\n",
    "                if col_names[fid] not in ['encounter_id', 'cuis']:\n",
    "                    if col_names[fid] in across_case_counts[f'case_{i}']['comte'].keys():\n",
    "                        across_case_counts[f'case_{i}']['comte'][col_names[fid]] += 1\n",
    "                    else:\n",
    "                        across_case_counts[f'case_{i}']['comte'][col_names[fid]] = 1\n",
    "\n",
    "        ################################### WINDOWSHAP ###################################\n",
    "        wshap_folder = list_of_wshap_folders[foldernum]\n",
    "        tf = open(f\"{wshap_folder}/{ACTUAL_EXP_NAME}_encounter_1000000{i}.0explanation.pkl\", \"rb\")\n",
    "        if IS_WSHAP_DICT:\n",
    "            wshap = pickle.load(tf)['wshap'][0]\n",
    "        else:\n",
    "            wshap = pickle.load(tf)[0]\n",
    "            wshap = wshap - wshap.mean(axis=0)\n",
    "        wshap = numpy.abs(wshap)[:, 3:-2]\n",
    "        tot_wshap = wshap.sum(axis=0)\n",
    "\n",
    "        top10 = numpy.argpartition(tot_wshap, -10)[-10:]\n",
    "        for top_id in top10:\n",
    "            if col_names[1:-1][top_id] in across_case_counts[f'case_{i}']['windowshap'].keys():\n",
    "                    across_case_counts[f'case_{i}']['windowshap'][col_names[1:-1][top_id]] += 1\n",
    "            else:\n",
    "                across_case_counts[f'case_{i}']['windowshap'][col_names[1:-1][top_id]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d71561",
   "metadata": {},
   "outputs": [],
   "source": [
    "for case_id in range(0,N_CASES):\n",
    "    case_counts = across_case_counts[f'case_{case_id}']\n",
    "    matches = []\n",
    "    dub_matches = set()\n",
    "    m_list = ['anchors','comte','windowshap']\n",
    "    for k in range(len(m_list)):\n",
    "        method_exp = m_list.pop(0)\n",
    "        case_method_vars = case_counts[method_exp].keys()\n",
    "        other_methods = [x for x in m_list if x != method_exp]\n",
    "        for v_name in case_method_vars:\n",
    "            for other_method in other_methods:\n",
    "                if v_name in case_counts[other_method].keys():\n",
    "                    if v_name in matches:\n",
    "                        dub_matches.add(v_name)\n",
    "                        matches.remove(v_name)\n",
    "                    elif v_name not in dub_matches:\n",
    "                        matches.append(v_name)\n",
    "    singles = [x for x in matches if x not in dub_matches]\n",
    "    print(f\"Case {case_id} - In Three: {dub_matches} - In Two: {matches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86cd93e",
   "metadata": {},
   "source": [
    "# PER CASE BEST OVERLAPPING EXAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e878a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading best explanations\n",
    "\n",
    "by_case_counts = {}\n",
    "for casenum in range(0,N_CASES):\n",
    "    ml = {}\n",
    "    for methodtype in ['anchors','comte','windowshap']:\n",
    "        lf = {}\n",
    "        for f_num, c_id in enumerate(list_of_comte_folders):\n",
    "            if methodtype == 'anchors':\n",
    "                a_id = list_of_anch_folders[f_num]\n",
    "                lf[a_id] = set()\n",
    "            elif methodtype == 'windowshap':\n",
    "                w_id = list_of_wshap_folders[f_num]\n",
    "                lf[w_id] = set()\n",
    "            else:\n",
    "                lf[c_id] = set()\n",
    "        ml[methodtype] = lf\n",
    "    by_case_counts[casenum] = ml\n",
    "\n",
    "filtered_col_names2 = copy.deepcopy(col_names)\n",
    "filtered_col_names2.remove(\"encounter_id\")\n",
    "filtered_col_names2.remove(\"cuis\")\n",
    "col_name_to_id = {v:k for k,v in enumerate(filtered_col_names2)}\n",
    "\n",
    "for i in range(0, N_CASES):\n",
    "    for foldernum, loc_folder in enumerate(list_of_comte_folders):\n",
    "        ################################### Anchors ###################################\n",
    "        anch_folder = list_of_anch_folders[foldernum]\n",
    "        tf = open(f\"{anch_folder}/Anchors_encounter_1000000{i}.0explanation.pkl\", \"rb\")\n",
    "        anchor = pickle.load(tf)[0]\n",
    "\n",
    "        ex_name = loc_folder\n",
    "        for acn in anchor['names']:\n",
    "            f_name = acn.split(\" \")[0]\n",
    "            f_name = \"_\".join(f_name.split(\"_\")[:-1])\n",
    "            if f_name not in ['encounter_id', 'cuis', 'hours_since_admit', 'timeofday', 'age']:\n",
    "                by_case_counts[i]['anchors'][anch_folder].add(f_name)\n",
    "\n",
    "        ################################### COMTE ###################################\n",
    "        curid = i #int_to_id_map[i]\n",
    "        matching_records = find_records(temp['combined_dat'], [curid])\n",
    "        matching_records['encounter_id'] = i\n",
    "        orig = matching_records.to_numpy()\n",
    "\n",
    "        tf = open(f\"{loc_folder}/CoMTE_encounter_1000000{i}.0explanation.pkl\", \"rb\")\n",
    "        comte = pickle.load(tf)[0].squeeze()\n",
    "        comte = numpy.concatenate([orig[:,:1], comte, orig[:,-1:]], axis=1)\n",
    "\n",
    "        diffs = numpy.abs(numpy.sum(orig - comte, axis=0))\n",
    "        for fid, f in enumerate(diffs):\n",
    "            if f > 0.0001:\n",
    "                if col_names[fid] not in ['encounter_id', 'cuis', 'hours_since_admit', 'timeofday', 'age']:\n",
    "                    by_case_counts[i]['comte'][ex_name].add(col_names[fid])\n",
    "\n",
    "        ################################### WINDOWSHAP ###################################\n",
    "        wshap_folder = list_of_wshap_folders[foldernum]\n",
    "        tf = open(f\"{wshap_folder}/{ACTUAL_EXP_NAME}_encounter_1000000{i}.0explanation.pkl\", \"rb\")\n",
    "        if IS_WSHAP_DICT:\n",
    "            wshap = pickle.load(tf)['wshap'][0]\n",
    "        else:\n",
    "            wshap = pickle.load(tf)[0]\n",
    "            wshap = wshap - wshap.mean(axis=0)\n",
    "        wshap = numpy.abs(wshap)[:, 2:-2]\n",
    "        tot_wshap = wshap.sum(axis=0)\n",
    "\n",
    "        top10 = numpy.argpartition(tot_wshap, -10)[-10:]\n",
    "        additional_loc = 1\n",
    "        drop_ids = [col_name_to_id[x] for x in ['encounter_id', 'cuis', 'hours_since_admit', 'timeofday', 'age'] if x in col_name_to_id.keys()]\n",
    "        for f_to_drop in ['encounter_id', 'cuis', 'hours_since_admit', 'timeofday', 'age']:\n",
    "            if f_to_drop in col_name_to_id.keys():\n",
    "                fd_id = col_name_to_id[f_to_drop]\n",
    "                if fd_id in top10: \n",
    "                    new_val = numpy.argpartition(tot_wshap, -10-additional_loc)[-10-additional_loc]\n",
    "                    additional_loc = additional_loc + 1\n",
    "                    while new_val in top10 or new_val in drop_ids:\n",
    "                        new_val = numpy.argpartition(tot_wshap, -10-additional_loc)[-10-additional_loc]\n",
    "                        additional_loc = additional_loc + 1\n",
    "\n",
    "                    top10[top10 == fd_id] = new_val                    \n",
    "        \n",
    "        for top_id in top10:\n",
    "            if col_names[1:-1][top_id] not in ['encounter_id', 'cuis', 'hours_since_admit', 'timeofday', 'age']:\n",
    "                by_case_counts[i]['windowshap'][wshap_folder].add(col_names[1:-1][top_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd83e2",
   "metadata": {},
   "source": [
    "# Get the maximum overlapping cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_best_combs = []\n",
    "for caseid, mvals in by_case_counts.items():\n",
    "    max_intersect_size = 0\n",
    "    best_secondary_intersects = 0\n",
    "    best_total_set_size = 999\n",
    "    best_comb = None\n",
    "    possibles = []\n",
    "    best_anchor_coverage = 0\n",
    "    \n",
    "    all_anch_coverages = []\n",
    "    for mname, explanationX in mvals.items():\n",
    "        v_tups = []\n",
    "        for tmpid, val in list(explanationX.items()):\n",
    "            v_tups.append((tmpid, val))\n",
    "        possibles.append(v_tups)\n",
    "        \n",
    "        if mname == 'anchors':\n",
    "            for case_loc in list_of_anch_folders:\n",
    "                tf = open(f\"{anch_folder}/Anchors_encounter_1000000{i}.0explanation.pkl\", \"rb\")\n",
    "                anchor = pickle.load(tf)\n",
    "                all_anch_coverages.append(anchor[0]['coverage'][-1])\n",
    "            \n",
    "    every_combo = list(itertools.product(*possibles))\n",
    "    for comb in every_combo:\n",
    "        comb[0][1].discard('outcome_ward24hr')\n",
    "        comb[1][1].discard('outcome_ward24hr')\n",
    "        comb[2][1].discard('outcome_ward24hr')\n",
    "        total_intersect = comb[0][1].intersection(comb[1][1]).intersection(comb[2][1])\n",
    "        total_union = comb[0][1].union(comb[1][1]).union(comb[2][1])\n",
    "        secondary_intersect_count = len(comb[0][1].intersection(comb[1][1])) + len(comb[0][1].intersection(comb[2][1])) + len(comb[2][1].intersection(comb[1][1])) - (3 * len(total_intersect))        \n",
    "        \n",
    "        loc_id = comb[0][0].split(\"/\")[1][4:]\n",
    "        if loc_id == '':\n",
    "            loc_id = 0\n",
    "        else:\n",
    "            loc_id = int(loc_id)\n",
    "        anchor_coverage = all_anch_coverages[loc_id]\n",
    "        \n",
    "        if len(total_intersect) > max_intersect_size:\n",
    "            best_comb = comb\n",
    "            max_intersect_size = len(total_intersect)\n",
    "            best_total_set_size = len(total_union)\n",
    "            best_secondary_intersects = secondary_intersect_count\n",
    "            best_anchor_coverage = anchor_coverage\n",
    "        elif (len(total_intersect) == max_intersect_size) and (secondary_intersect_count > best_secondary_intersects):\n",
    "            best_comb = comb\n",
    "            max_intersect_size = len(total_intersect)\n",
    "            best_total_set_size = len(total_union)\n",
    "            best_secondary_intersects = secondary_intersect_count\n",
    "            best_anchor_coverage = anchor_coverage\n",
    "        elif (len(total_intersect) == max_intersect_size) and (secondary_intersect_count == best_secondary_intersects) and (len(total_union) < best_total_set_size):\n",
    "            best_comb = comb\n",
    "            max_intersect_size = len(total_intersect)\n",
    "            best_total_set_size = len(total_union)\n",
    "            best_secondary_intersects = secondary_intersect_count\n",
    "            best_anchor_coverage = anchor_coverage\n",
    "        elif (len(total_intersect) == max_intersect_size) and (secondary_intersect_count == best_secondary_intersects) and (len(total_union) == best_total_set_size) and anchor_coverage > best_anchor_coverage:\n",
    "            best_comb = comb\n",
    "            max_intersect_size = len(total_intersect)\n",
    "            best_total_set_size = len(total_union)\n",
    "            best_secondary_intersects = secondary_intersect_count\n",
    "            best_anchor_coverage = anchor_coverage\n",
    "            \n",
    "    print(f\"Case {caseid} best comb {best_comb} with primary {max_intersect_size} secondary {best_secondary_intersects} total_len {best_total_set_size}\\n\")\n",
    "    all_best_combs.append(best_comb) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93dfdb6",
   "metadata": {},
   "source": [
    "# CHECKING CASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d1483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for manual validation of cases\n",
    "\n",
    "filtered_col_names = copy.deepcopy(col_names)\n",
    "filtered_col_names.remove(\"encounter_id\")\n",
    "filtered_col_names.remove(\"cuis\")\n",
    "col_name_to_id = {v:k for k,v in enumerate(filtered_col_names)}\n",
    "print(col_name_to_id)\n",
    "\n",
    "\n",
    "how_many_window_shap_fs = 5\n",
    "\n",
    "\n",
    "for i, case_data in enumerate(all_best_combs):\n",
    "    anchor_folder = case_data[0][0]\n",
    "    comte_folder = case_data[1][0]\n",
    "    windowshap_folder = case_data[2][0]\n",
    "\n",
    "    anch_fs = case_data[0][1]\n",
    "    comt_fs = case_data[1][1]\n",
    "    wshp_fs = case_data[2][1]\n",
    "    print(f\"SAFE CHECK: {anch_fs}\")\n",
    "    \n",
    "    curid = i\n",
    "    matching_records = find_records(temp['combined_dat'], [curid])\n",
    "    orig = matching_records.to_numpy()[:, 1:-1]\n",
    "    orig_time = matching_records['hours_since_admit'].to_numpy()\n",
    "    print(f\"ORIG SHAPE: {orig.shape}\")\n",
    "    print(f\"ORIG TIMES: {orig_time}\")\n",
    "    \n",
    "    #############################################################################################\n",
    "    tf = open(f\"{anchor_folder}/Anchors_encounter_1000000{i}.0explanation.pkl\", \"rb\")\n",
    "    anchor = pickle.load(tf)[0]\n",
    "    \n",
    "\n",
    "    #############################################################################################\n",
    "    tf = open(f\"{comte_folder}/CoMTE_encounter_1000000{i}.0explanation.pkl\", \"rb\")\n",
    "    comte = pickle.load(tf)[0].squeeze()\n",
    "    print(f\"COMTE SHAPE: {comte.shape}\")\n",
    "    \n",
    "    #############################################################################################\n",
    "    tf = open(f\"{windowshap_folder}/{ACTUAL_EXP_NAME}_encounter_1000000{i}.0explanation.pkl\", \"rb\")\n",
    "    if IS_WSHAP_DICT:\n",
    "        wshap = pickle.load(tf)['wshap'][0]\n",
    "    else:\n",
    "        wshap = pickle.load(tf)[0]\n",
    "        wshap = wshap - wshap.mean(axis=0)\n",
    "    \n",
    "    print(f\"wshap: {wshap.shape}\")\n",
    "    tot_wshap = wshap.sum(axis=0) / wshap.sum()\n",
    "    print(f\"wshap: {wshap.shape} tot_wshap:{tot_wshap.shape} numpy.argsort(tot_wshap):{numpy.argsort(tot_wshap)}\")\n",
    "    W_feat_sorted = np.expand_dims(numpy.argsort(tot_wshap), axis=0)[::-1]\n",
    "    \n",
    "    which_wshap_in_others = []\n",
    "    print(f\"CHECKING wshp_fs: {wshp_fs}\")\n",
    "    for feat in wshp_fs:\n",
    "        if feat in anch_fs.union(comt_fs):\n",
    "            which_wshap_in_others.append(feat)\n",
    "    check_idx = 0\n",
    "    list_wshap_fs = list(wshp_fs)\n",
    "    while len(which_wshap_in_others) < how_many_window_shap_fs:\n",
    "        if list_wshap_fs[check_idx] not in which_wshap_in_others:\n",
    "            which_wshap_in_others.append(list_wshap_fs[check_idx])\n",
    "        check_idx += 1\n",
    "    final_windowshap_names = which_wshap_in_others\n",
    "    \n",
    "    #############################################################################################\n",
    "    all_fs = anch_fs.union(comt_fs).union(set(final_windowshap_names))\n",
    "    subset_orig = []\n",
    "    all_fs_ids = []\n",
    "    print(f\"ALL FEATURES: {all_fs}\")\n",
    "    print(f\"ALL FEATURES A: {anch_fs}\")\n",
    "    print(f\"ALL FEATURES C: {comt_fs}\")\n",
    "    print(f\"ALL FEATURES W: {final_windowshap_names}\")\n",
    "    \n",
    "    for feat in all_fs:\n",
    "        f_id = col_name_to_id[feat]\n",
    "        all_fs_ids.append(f_id)\n",
    "        subset_orig.append(orig[:, f_id])\n",
    "    final_orig = np.stack(subset_orig, axis=1)\n",
    "    \n",
    "    comte_orig_data = []\n",
    "    comte_fs_names = []\n",
    "    comte_cf = []\n",
    "    for comte_f in comt_fs:\n",
    "        f_id = col_name_to_id[comte_f]\n",
    "        comte_fs_names.append(comte_f)\n",
    "        comte_orig_data.append(orig[:, f_id])\n",
    "        comte_cf.append(comte[:, col_names.index(comte_f)])\n",
    "    print(f\"TESTING: {comte.shape} vs {len(col_names)} vs {orig.shape}\")\n",
    "    new_c = np.stack(comte_cf, axis=1)\n",
    "    new_comte_o = np.stack(comte_orig_data, axis=1)\n",
    "\n",
    "    ############################################################################################\n",
    "    final_windowshap = []\n",
    "    for f in all_fs:\n",
    "        try:\n",
    "            final_windowshap.append(wshap[:,col_name_to_id[f]])\n",
    "        except:\n",
    "            print(f\"failed on {f} wshap: {wshap.shape} \\n colnames: {col_name_to_id}\")\n",
    "    final_windowshap = np.stack(final_windowshap)    \n",
    "    \n",
    "    #############################################################################################\n",
    "    anch_expl = np.zeros_like(orig)\n",
    "    print(f\"ANCHOR: {anchor['names']} against matching: {anch_fs}\")\n",
    "    filtered_anchor = []\n",
    "    for feat_sign_val in anchor['names']:\n",
    "        feat_time_name, sign, val = feat_sign_val.split(\" \")\n",
    "        f_name = \"_\".join(feat_time_name.split(\"_\")[:-1])\n",
    "        if f_name not in ['cuis', 'encounter_id', 'outcome_ward24hr', 'hours_since_admit', 'timeofday', 'age']:\n",
    "            timepoint = feat_time_name.split(\"_\")[-1]\n",
    "            filtered_anchor.append(f\"{f_name} at hour {int(timepoint)} {sign} {val}\")\n",
    "            \n",
    "            if f_name not in anch_fs:\n",
    "                raise NotImplementedError(f\"ANCHOR FAIL ON {f_name} NOT IN {anch_fs}\")\n",
    "            fid = list(all_fs).index(f_name)\n",
    "            anch_expl[fid, int(timepoint)] = val\n",
    "    \n",
    "    \n",
    "    #############################################################################################\n",
    "    new_c = np.expand_dims(new_c, axis=0)\n",
    "    new_w = np.expand_dims(final_windowshap.transpose(), axis=0)\n",
    "    allvals = temp['combined_dat'][list(all_fs)]\n",
    "\n",
    "    print(f\"PRINT comte shape: {comte.shape} and final: {final_orig.shape} \"\\\n",
    "          f\" and new_c_min_val: {new_c.shape} and {new_w.shape}\" \\\n",
    "          f\"{len(comte_fs_names)} and {len(all_fs)}\")\n",
    "    min_max_dict = {}\n",
    "    \n",
    "    for f_id, f_name in enumerate(list(all_fs)):\n",
    "        if f_name in comte_fs_names:\n",
    "            c_id = comte_fs_names.index(f_name)\n",
    "            cf_min_val = new_c[:, :, c_id].min()\n",
    "            cf_max_val = new_c[:, :, c_id].max()\n",
    "        else:\n",
    "            cf_min_val = 9999999\n",
    "            cf_max_val = -1\n",
    "            \n",
    "        orig_min_val = final_orig[:, f_id].min()\n",
    "        orig_max_val = final_orig[:, f_id].max()\n",
    "                \n",
    "        overall_max = max(orig_max_val, cf_max_val)\n",
    "        overall_min = min(orig_min_val, cf_min_val)\n",
    "        range_change = overall_max - overall_min\n",
    "        overall_max = overall_max + (0.10 * range_change)\n",
    "        overall_min = overall_min - (0.10 * range_change)\n",
    "\n",
    "        print(f\"MAXES: {orig_max_val} vs {cf_max_val} and {orig_min_val} vs  {cf_min_val}\")\n",
    "        \n",
    "        min_max_dict[f_name] = (overall_min, overall_max)\n",
    "    print(f\"PRINT MINMAX_DICT: {min_max_dict}\\n {new_comte_o}\\n\")\n",
    "    plot_original_overlap_counterfactual(np.expand_dims(new_comte_o, axis=0), \n",
    "                                         [new_c], \n",
    "                                         comte_fs_names, \n",
    "                                         'viz_outs/', \n",
    "                                         image_name_prefix=f\"cf_{i}_\", \n",
    "                                         n_plots_horiz=1,\n",
    "                                         min_max_dict=min_max_dict,\n",
    "                                         display_names=display_names,\n",
    "                                         timepoints=orig_time)\n",
    "    \n",
    "    plot_original_line_with_vals(np.expand_dims(final_orig, axis=0), \n",
    "                                 new_w, \n",
    "                                 list(all_fs), \n",
    "                                 'viz_outs/', \n",
    "                                 image_name_prefix=f\"ws_{i}_\", \n",
    "                                 n_plots_horiz=1,\n",
    "                                 min_max_dict=min_max_dict,\n",
    "                                 display_names=display_names,\n",
    "                                 timepoints=orig_time)\n",
    "    \n",
    "    with open(f'viz_outs/ac_{i}.txt', \"w\") as anchor_file:\n",
    "        subtext = ' AND\\n'.join([\"\\t\"+x for x in filtered_anchor])\n",
    "        formatted_text = f\"BECAUSE: \\n {subtext} \\nTHEN:\\n\\tThis patient is expected to be at high risk of clinical deterioration.\"\n",
    "        anchor_file.write(formatted_text)\n",
    "    anchor_file.close()\n",
    "    print(\"--------------------------\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2fcbe6",
   "metadata": {},
   "source": [
    "# Write synthetic data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e83d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_case_superset = {}\n",
    "for i, case_data in enumerate(all_best_combs):\n",
    "    superset = set()\n",
    "    for item in case_data:\n",
    "        superset = superset.union(item[1])\n",
    "    per_case_superset[i] = superset\n",
    "\n",
    "combo_set = set()\n",
    "for k,v in per_case_superset.items():\n",
    "    combo_set = combo_set.union(v)\n",
    "    \n",
    "print(per_case_superset)\n",
    "print(\"---------------------\")\n",
    "print(combo_set)\n",
    "print(len(combo_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdce1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_disp = {\n",
    " 'hours_since_admit': 'Hours Since Admit',\n",
    " 'hr': 'Heart rate',\n",
    " 'rr': 'Respiratory rate',\n",
    " 'sbp': 'SBP',\n",
    " 'dbp': 'DBP',\n",
    " 'o2sat': 'Oxygen saturation',\n",
    " 'temp_c': 'Temperature (Celsius)',\n",
    " 'avpu': 'AVPU mental status',\n",
    " 'disoriented': 'Disoriented',\n",
    " 'bmi': 'BMI',\n",
    " 'fio2_final': 'Delivered FiO2',\n",
    " 'braden_activity': 'Braden Scale - Activity',\n",
    " 'braden_friction': 'Braden Scale - Friction',\n",
    " 'braden_mobility': 'Braden Scale - Mobility',\n",
    " 'braden_moisture': 'Braden Scale - Moisture',\n",
    " 'braden_nutrition': 'Braden Scale - Nutrition',\n",
    " 'braden_sensory': 'Braden Scale - Sensory',\n",
    " 'braden_scale': 'Sum Total of Braden Scale',\n",
    " 'urine_output_sum': 'Urine output sum over 24 hours',\n",
    "}\n",
    "labs_disp = {\n",
    " 'hours_since_admit': 'Hours Since Admit',\n",
    " 'albumin': 'Albumin',\n",
    " 'alk_phos': 'Alkaline phosphatase',\n",
    " 'anion_gap': 'Anion Gap',\n",
    " 'bands_pct': 'Bands',\n",
    " 'bili_total': 'Total bilirubin',\n",
    " 'bun': 'BUN',\n",
    " 'calcium': 'Calcium',\n",
    " 'chloride': 'Chloride',\n",
    " 'co2': 'CO2',\n",
    " 'creatinine': 'Creatinine',\n",
    " 'eosinophils_pct': 'Eosinophils',\n",
    " 'gluc_ser': 'Glucose',\n",
    " 'hb': 'Hemoglobin',\n",
    " 'inr': 'INR',\n",
    " 'lactate': 'Lactate',\n",
    " 'lipase': 'Lipase',\n",
    " 'lymphocytes_pct': 'Lymphocytes',\n",
    " 'magnesium': 'Magnesium',\n",
    " 'mcv': 'Mean Corpuscular Volume',\n",
    " 'monocytes_pct': 'Monocytes',\n",
    " 'neutrophils_pct': 'Neutrophils',\n",
    " 'pco2_art': 'PCO2 (Arterial)',\n",
    " 'pco2_ven': 'PCO2 (Venous)',\n",
    " 'ph_art': 'pH (Arterial)',\n",
    " 'ph_ven': 'pH (Venous)',\n",
    " 'phosphate': 'Inorganic Phosphate',\n",
    " 'platelet_count': 'Platelets',\n",
    " 'po2_art': 'PO2 (Arterial)',\n",
    " 'potassium': 'Potassium',\n",
    " 'ptt': 'PTT',\n",
    " 'rdw': 'RBC Distribution Width',\n",
    " 'sgot': 'SGOT',\n",
    " 'sodium': 'Sodium',\n",
    " 'total_protein': 'Total Protein',\n",
    " 'wbc': 'WBC count',\n",
    " 'age': 'Age'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeabcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map = {v:k for k,v in datafile['orig_dat_map'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_nan_placeholder = -98989898989\n",
    "for i, case_data in enumerate(all_best_combs):\n",
    "    \n",
    "    anchor_folder = case_data[0][0]    \n",
    "    comte_folder = case_data[1][0]\n",
    "    windowshap_folder = case_data[2][0]\n",
    "\n",
    "    anch_fs = case_data[0][1]\n",
    "    comt_fs = case_data[1][1]\n",
    "    wshp_fs = case_data[2][1]\n",
    "    \n",
    "    curid = i\n",
    "    matching_records = find_records(datafile['orig_dat_noise'], [inv_map[i]])\n",
    "    print(f\"MATCH SIZE: {matching_records.shape}\")\n",
    "    orig = pd.DataFrame(matching_records.to_numpy()[:, 1:-1], columns=filtered_col_names)\n",
    "    orig = orig.fillna(temp_nan_placeholder)\n",
    "    \n",
    "    #do rounding\n",
    "    mins = ((orig['hours_since_admit'] - np.floor(orig['hours_since_admit'])) * 60).round(decimals=2).astype(int)\n",
    "    hours = np.floor(orig['hours_since_admit']).astype(int)\n",
    "    strs = [f\"{hours[i]}h {mins[i]}m\" for i in range(mins.shape[0])]\n",
    "    orig['hours_since_admit'] = strs\n",
    "    orig['temp_c'] = np.floor(orig['temp_c'])\n",
    "    orig['bmi'] = np.floor(orig['bmi'])\n",
    "    orig['mcv'] = orig['mcv'].astype(float).round(2)\n",
    "    orig['ptt'] = orig['ptt'].astype(float).round(2)\n",
    "    orig['rdw'] = orig['rdw'].astype(float).round(2)\n",
    "    \n",
    "    #replace nan with str\n",
    "    orig[orig == temp_nan_placeholder] = '-'\n",
    "    \n",
    "    subset = copy.deepcopy(orig)\n",
    "    subset_labs = copy.deepcopy(orig)\n",
    "    subset_vits = copy.deepcopy(orig)\n",
    "    \n",
    "    orig = orig.transpose()\n",
    "    orig.to_csv(f\"viz_outs/synthetic_case_{i}.csv\", header=False)\n",
    "    print(f\"Case {i}: \\n{orig}\")\n",
    "    \n",
    "    used_cols = ['hours_since_admit'] + list(combo_set)\n",
    "    subset = subset[used_cols]\n",
    "    subset = subset.transpose()\n",
    "    subset.to_csv(f\"viz_outs/synthetic_anysubset_{i}.csv\", header=False)\n",
    "    \n",
    "    #############################################\n",
    "    used_cols_labs = ['hours_since_admit'] + [x for x in list(combo_set) if x in labs_disp.keys()]\n",
    "    subset_labs = subset_labs[used_cols_labs]\n",
    "    subset_labs.columns = [labs_disp[x] for x in used_cols_labs]\n",
    "    subset_labs = subset_labs.transpose()\n",
    "    subset_labs.to_csv(f\"viz_outs/synthetic_labssubset_{i}.csv\", header=False)\n",
    "    \n",
    "    used_cols_vits = ['hours_since_admit'] + [x for x in list(combo_set) if x in vitals_disp.keys()]\n",
    "    subset_vits = subset_vits[used_cols_vits]\n",
    "    subset_vits.columns = [vitals_disp[x] for x in used_cols_vits]\n",
    "    subset_vits = subset_vits.transpose()\n",
    "    subset_vits.to_csv(f\"viz_outs/synthetic_vitssubset_{i}.csv\", header=False)\n",
    "    \n",
    "    print(\"================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb050338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
